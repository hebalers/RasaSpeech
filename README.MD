# Adding custom actions to Google Assistant with Rasa

This repository contains the following files and directories:
- **rasa** - this directory contains a Rasa assistant named Rasa Speech.
- **README.MD** - This README-file.
- **action.json** - a configuration file for custom Google Assistant actions.
- **ga_connector.py** - Rasa-Google Assistant connector. 

## Rasa

To extend Google Assistant functionality with Rasa you need a Rasa assistant. To familiarize yourself with Rasa you can follow the [Getting Started with Rasa-tutorials](https://rasa.com/docs/getting-started/). After following these tutorials you should have a Rasa Assistant with trained NLU and dialogue models. These models are responsible for understanding what the user says and determining the appropriate response. 

## Google Actions Console

The Google Actions Console enables Google Assistant's speech-to-text service to enable voice control and audible feedback for your Rasa assistant. Initialization of the custom Google Assistant custom action is required. This can be done by going to [this page](https://console.actions.google.com/) and selecting 'Add/import project'. 

Fill in the required fields and choose the 'Actions SDK' category. The next window, which was opened after selecting Actions SDK, you will see a link to the Google Actions SDK guide and a gactions function. This gactions function is used to deploy any custom action. 

The next step is to tell Google Actions how the custom skill should be invoked. Click 'Decide how your Action is invoked' in the 'Quick Setup'-pane. You can also select the display name that is used to start a custom skill and the voice Google Assistant should use for your assistant.

## Creating the Rasa custom action

Rasa takes care of NLU and dialogue managements using the trained NLU and Core Models. Therefore, custom Google assistant behaviour will be responsible for only two things:
- **understanding that a custom action was invoked**
- **passing all user input to Rasa after invocation** 

To achieve this you will have to define intents for these two behaviours and fulfillments - things that the Google Assistant will have to do once these intents are matched after performing speech-to-text on user voice inputs.

Information on implenting Rasa custom actions can be found [here](https://rasa.com/docs/rasa/core/actions/).

## Ngrok

Ngrok is tunnelling/reverse proxy software that establishes secure tunnels from a public endpoint such as the internet to a locally running network service while capturing all traffic for detailed inspection and replay. You can find more information on Ngrok [here](https://vmokshagroup.com/blog/expose-your-localhost-to-web-in-50-seconds-using-ngrok/). 

You can sign up for Ngrok at [ngrok.com](https://ngrok.com).

## Running the project

Start the Rasa server with the command below.

```rasa run -vv --enable-api -p 5004```

The Rasa assistant runs locally and because Google Assistant does not, we need Ngrok as a tunnelling service to expose the local Rasa assistant to remote services. Start Ngrok on the same port as the local Rasa assistant, port 5004. 

```ngrok http 5004```

Be sure to update the fulfillments in the Google Assistant custom action to the webhook URL. Do so by copying the url generated by ngrok (shown in the ngrok terminal), attach '/webhooks/google_assistant/webhook' to it and providef the url to the urls under the conversations object. An example is shown below:

```{
  "actions": [
    {
      "description": "Default Welcome Intent",
      "name": "MAIN",
      "fulfillment": {
        "conversationName": "welcome"
      },
      "intent": {
        "name": "actions.intent.MAIN",
        "trigger": {
          "queryPatterns": ["talk to Rasa Speech"]
        }
      }
    },
    {
      "description": "Rasa Intent",
      "name": "TEXT",
      "fulfillment": {
        "conversationName": "rasa_intent"
      },
      "intent": {
        "name": "actions.intent.TEXT",
        "trigger": {
          "queryPatterns": []
        }
      }
    }],
  "conversations": {
    "welcome": {
      "name": "welcome",
      "url": "https://4f331103.ngrok.io/webhooks/google_assistant/webhook",
      "fulfillmentApiVersion": 2
    },
    "rasa_intent": {
      "name": "rasa_intent",
      "url": "https://4f331103.ngrok.io/webhooks/google_assistant/webhook",
      "fulfillmentApiVersion": 2
    }
  }
}
```

**You can now also use a Google Home device or a Google Assistant app to test any custom Google Assistant skills, as long as the device/app is signed in to the same Google Account as the one that configured the custom action.**

If your Rasa assistant includes Rasa custom actions (see [here](https://rasa.com/docs/rasa/core/actions/)), also start the actions server by running the following command:

```rasa run actions```

If you have DucklingHTTPExtractor in your NLP pipeline (see [config.yml](../rasa/config.yml) ,start a local server for it as well. You can do it in a Docker container by running a command below:

```docker run -p 8000:8000 rasa/duckling```

Now, all that is left to do is to deploy your custom Google Assistant skill and enable testing. You can deploy the custom Google Assistant skill by running the command below.

```gactions update --action_package action.json --project rasa-speech```

Once the action is uploaded, enable testing of the action by running the following command:

```gactions test --action_package action.json --project rasa-speech```

**If you get an error that gaconnector.py module doesn't exist, add your project to the python path by running: export PYTHONPATH=/pathtorasaspeech/:$PYTHONPATH**
